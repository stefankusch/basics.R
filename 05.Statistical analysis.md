**R** offers a lot of statistical tests, basically enabling all types of testing, be it parametric or non-parametric tests, pair-wise testing or multiple regression, and so on. You will find a solution for almost every statistical problem in the internet (e.g. see Useful links). For the kind of datasets that are produced very often in pathogen assays, pair-wise testing makes sense. In addition, one has to keep in mind that (a) the datasets are almost always very small (less than 15 independent replicates), and (b) in most cases do not show normal distribution.

#### Preparing dataset
As described in part 03 (Formatting the data), the dataset should be transformed to look like this:
```R
> head(data.melt)
     sample value
1 wild.type    71
2 wild.type    74
3 wild.type    80
4 wild.type    89
5   mutant1     0
6   mutant1     1
```

Now we can use `transform()` for any changes necessary. Let's assume that the data might be ratio type with 100 as maximum. 
```R
> data.melt <- transform(data.melt, 
+ res = 100-value)
> head(data.melt)
     sample value res
1 wild.type    71  29
2 wild.type    74  26
3 wild.type    80  20
4 wild.type    89  11
5   mutant1     0 100
6   mutant1     1  99
```

### Testing for normal distribution

#### 1. Visual methods
There are two major types of visual data representations that can help assess the data distribution pattern, density plots and Q-Q plots. The methods below rely on the package `ggpubr` (see documentation at https://rpkgs.datanovia.com/ggpubr/), but these plots can also be generated with `ggplots2` or base-R. 

##### Density plot
A density plot is a representation of the distribution of a numeric variable. Density plots can be generated with the function `ggdensity()`. Using the example data above:
```
plot.density <- ggdensity(data.melt$value, 
          main = "Density plot",
          xlab = "Penetration rate [%]")
```
The resulting plot might look similar to this:
![plot.density](https://github.com/stefankusch/basics.R/blob/master/density_01.png)

By comparision, a normal distribution looks like this:
![plot.density.nd](https://github.com/stefankusch/basics.R/blob/master/density_02.png)

##### Q-Q plot
Another method to inspect data distribution is the quantile-quantile (Q-Q) plot, which is a scatterplot created by plotting two sets of quantiles against one another. The function `ggqqplot()` is a simple and effective way to generate a Q-Q plot:
```
plot.qq <- ggqqplot(data.melt$value)
```
This generates a plots like this:
![plot.qq](https://github.com/stefankusch/basics.R/blob/master/qq_01.png)

Compared to a Q-Q plot generated with normally distributed data:
![plot.qq.nd](https://github.com/stefankusch/basics.R/blob/master/qq_02.png)

#### 2. Normality test (Shapiro-Wilk method)
In addition to visual approaches, statistical tests can be used to determine if a dataset exhibits normal distribution. One of the most popular tests is the Shapiro-Wilk test, which is a hypothesis test where the null hypothesis states that the generated data is normally distributed. Accordingly, if `*P* < 0.05`, the null hypothesis can be rejected and the data is unlikely to exhibit normal distribution.
```
shapiro.test(data.melt$value)
```
The test generates an output like this:
```R
	Shapiro-Wilk normality test

data:  data.melt$value
W = 0.95422, p-value = 0.0008357
```
In this case, we observe a distribution different from normal.

#### Wilcoxon-Mann-Whitney Rank Sum test (non-parametric)

```
wilcox.test(value[1:8] ~ sample[1:8], data=data.melt)
```
produces a significant difference!
```R
        Wilcoxon rank sum test with continuity correction

data:  value[1:8] by sample[1:8]
W = 16, p-value = 0.0294
alternative hypothesis: true location shift is not equal to 0

Warning message:
In wilcox.test.default(x = c(71, 74, 80, 89), y = c(0, 1, 0, 0.5) :
  cannot compute exact p-value with ties
```
This test requires at least 4 independent replicates, and only works pair-wise. There are certainly scripts to automate this for every pair.

#### GLM assuming Poisson distribution (good for datasets with continuous values, such as spore counts or enzyme activity tests)

```
model_glm <- glm(value~sample, family=poisson, data=data.melt)
summary(model_glm)
```
Let's look at the output:
```R
> summary(model_glm)

Call:
glm(formula = value ~ sample, family = poisson, data = data.melt)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1490  -0.8619   0.1132   0.3967   1.1601  

Coefficients:
                Estimate Std. Error z value Pr(>|z|)    
(Intercept)      4.36310    0.05643  77.314  < 2e-16 ***
genotypemutant1 -5.34393    0.81844  -6.529  6.6e-11 ***
genotypemutant2 -1.43190    0.12852 -11.141  < 2e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 463.4789  on 11  degrees of freedom
Residual deviance:   6.5655  on  9  degrees of freedom
AIC: Inf

Number of Fisher Scoring iterations: 5
```
`family=poisson` defines that the GLM should assume Poisson distribution of the data. We see that the **Residual deviance** is 6.5655 on 9 **degrees of freedom**, which is good. If the Residual deviance were much higher than the degrees of freedom, we should adjust the distribution to `family=quasipoisson`. Next, the `Coefficient` lines are interesting. The `(Intercept)` lines show the accuracy of the modelling of the default dataset, here wild.type. A p-value of < 2e-16 is very good: the lower the p-value, the better the model. Next, we see statistical estimations for mutant1 and mutant2, both compared to wild.type, and both are highly significantly different from wild.type.
To compare mutant1 with mutant2 simply use the line `model_glm <- glm(value[5:12]~sample[5:12], family=poisson, data=stat.data)` to select only the rows associated with these two genotypes.

#### GLM assuming binomial distribution (good for datasets with ratio or percentage values)

```
model_glm <- glm(cbind(value,res)~sample, family=binomial, data=data.melt)
``` 
The output:
```R
> summary(model_glm)

Call:
glm(formula = cbind(value, res) ~ sample, family = binomial, 
    data = data.melt)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7597  -0.9177   0.1292   0.4801   2.7572  

Coefficients:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)        1.2950     0.1217  10.641   <2e-16 ***
genotypemutant1   -6.8773     0.8270  -8.316   <2e-16 ***
genotypemutant2   -2.7614     0.1767 -15.628   <2e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 708.504  on 11  degrees of freedom
Residual deviance:  16.599  on  9  degrees of freedom
AIC: 61.954

Number of Fisher Scoring iterations: 5
```
The main difference is that we need to link failure and success values (here value and res) by `cbind(value,res)`, and use `family=binomial`. The output reads the same way as before. We can see that the p-values are even slightly better than with Poisson distribution, which makes sense for this ratio data.
