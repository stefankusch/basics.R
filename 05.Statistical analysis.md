**R** offers a lot of statistical tests, basically enabling all types of testing, be it parametric or non-parametric tests, pair-wise testing or multiple regression, and so on. You will find a solution for almost every statistical problem in the internet (e.g. see Useful links). For the kind of datasets that are produced very often in pathogen assays, pair-wise testing makes sense. In addition, one has to keep in mind that (a) the datasets are almost always very small (less than 15 independent replicates), and (b) in most cases do not show normal distribution.

#### Preparing dataset
As described in part 03 (Formatting the data), the dataset should be transformed to look like this:
```R
> head(data.melt)
     sample value
1 wild.type    71
2 wild.type    74
3 wild.type    80
4 wild.type    89
5   mutant1     0
6   mutant1     1
```

Now we can use `transform()` for any changes necessary. Let's assume that the data might be ratio type with 100 as maximum. 
```R
> data.melt <- transform(data.melt, 
+ res = 100-value)
> head(data.melt)
     sample value res
1 wild.type    71  29
2 wild.type    74  26
3 wild.type    80  20
4 wild.type    89  11
5   mutant1     0 100
6   mutant1     1  99
```

### Testing for normal distribution

#### 1. Visual methods
There are two major types of visual data representations that can help assess the data distribution pattern, density plots and Q-Q plots. The methods below rely on the package `ggpubr` (see documentation at https://rpkgs.datanovia.com/ggpubr/), but these plots can also be generated with `ggplots2` or base-R. 

##### Density plot
A density plot is a representation of the distribution of a numeric variable. Density plots can be generated with the function `ggdensity()`. Using the example data above:
```
plot.density <- ggdensity(data.melt$value, 
          main = "Density plot",
          xlab = "Penetration rate [%]")
```
The resulting plot might look similar to this:<br>
<img style="float: left;" src="https://github.com/stefankusch/basics.R/blob/master/density_01.png" data-canonical-src="https://github.com/stefankusch/basics.R/blob/master/density_01.png" width="400" height="400" />

By comparision, a normal distribution looks like this:<br>
<img style="float: left;" src="https://github.com/stefankusch/basics.R/blob/master/density_02.png" data-canonical-src="https://github.com/stefankusch/basics.R/blob/master/density_02.png" width="400" height="400" />

##### Q-Q plot
Another method to inspect data distribution is the quantile-quantile (Q-Q) plot, which is a scatterplot created by plotting two sets of quantiles against one another. The function `ggqqplot()` is a simple and effective way to generate a Q-Q plot:
```
plot.qq <- ggqqplot(data.melt$value)
```
This generates a plots like this:<br>
<img style="float: left;" src="https://github.com/stefankusch/basics.R/blob/master/qq_01.png" data-canonical-src="https://github.com/stefankusch/basics.R/blob/master/qq_01.png" width="400" height="400" />

Compared to a Q-Q plot generated with normally distributed data:<br>
<img style="float: left;" src="https://github.com/stefankusch/basics.R/blob/master/qq_02.png" data-canonical-src="https://github.com/stefankusch/basics.R/blob/master/qq_02.png" width="400" height="400" />

#### 2. Normality test (Shapiro-Wilk method)
In addition to visual approaches, statistical tests can be used to determine if a dataset exhibits normal distribution. One of the most popular tests is the Shapiro-Wilk test, which is a hypothesis test where the null hypothesis states that the generated data is normally distributed. Accordingly, if `*P* < 0.05`, the null hypothesis can be rejected and the data is unlikely to exhibit normal distribution.
```
shapiro.test(data.melt$value)
```
The test generates an output like this:
```R
	Shapiro-Wilk normality test

data:  data.melt$value
W = 0.95422, p-value = 0.0008357
```
In this case, we observe a distribution different from normal.


### Statistical tests (normal distribution)

#### Pairwise test: t-test
Pairwise tests are used to determine the statistical difference between ***two groups***. If the data exhibits normal distribution, a t-test can be used. A t-test calculates the likelihood that the null hypothesis, which states that the mean difference between two groups is equal or close to 0, is true. The null hypothesis is rejected if the *P* value is below alpha, where alpha often equals 0.05, 0.01, or 0.001. Depending on the variance of the two groups, and if the groups are paired or unpaired, different variations of the t-test are used.

##### Two-sample t-test (unequal variance: Welch test)

```
t.test(value ~ sample, data=data.melt)
```

##### Two-sample t-test (equal variance)

```
t.test(value ~ sample, data=data.melt, var.equal = TRUE)
```

##### Paired t-test

```
t.test(value ~ sample, data=data.melt, paired = TRUE)
```
The t-tests generate an output like this. 
```R
	Paired t-test

data:  value by sample
t = -45.485, df = 299, p-value < 2.2e-16
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -32.50442 -29.80842
sample estimates:
mean difference 
      -31.15642
```
This output contains important information needed to interpret the test, which are:
- **data**: Indicates the data used in the t-test (value and group).
- **t**: A t-test statistic. Here, the negative t-value indicates that the sample mean of group 1 is significantly smaller than the mean of group 2.
- **df**: The degree of freedom associated with the t-test value.
- **p-value**: Indicates the statistical significance of the result. A _P_-value below alpha (0.005) signifies that the probability of obtaining the observed difference between the two groups by chance is very small.
- **alternative hypothesis**: The alternative to the null hypothesis, here, that the true mean difference does not equal zero.
- **95 percent confidence interval**: The algorithm is 95% confident that the difference between the true population means lies within the range of these two values.
- **sample estimates**: This output provides information about the means of group 1 and group 2, here as mean difference.


#### Two or more groups: ANOVA
ANOVA (ANalysis Of VAriance) is a statistical test to determine whether two or more population means are different. ANOVA compares ***two or more groups*** for statistically significant differences by generalizing t-tests across multiple groups. As with t-tests, ANOVA relies on normally distributed data. 

##### testing


### Statistical tests (non-normal distribution)

#### Wilcoxon-Mann-Whitney Rank Sum test

```
wilcox.test(value[1:8] ~ sample[1:8], data=data.melt)
```
produces a significant difference!
```R
        Wilcoxon rank sum test with continuity correction

data:  value[1:8] by sample[1:8]
W = 16, p-value = 0.0294
alternative hypothesis: true location shift is not equal to 0

Warning message:
In wilcox.test.default(x = c(71, 74, 80, 89), y = c(0, 1, 0, 0.5) :
  cannot compute exact p-value with ties
```
This test requires at least 4 independent replicates, and only works pair-wise. There are certainly scripts to automate this for every pair.

#### GLM assuming Poisson distribution (good for datasets with continuous values, such as spore counts or enzyme activity tests)

```
model_glm <- glm(value~sample, family=poisson, data=data.melt)
summary(model_glm)
```
Let's look at the output:
```R
> summary(model_glm)

Call:
glm(formula = value ~ sample, family = poisson, data = data.melt)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1490  -0.8619   0.1132   0.3967   1.1601  

Coefficients:
                Estimate Std. Error z value Pr(>|z|)    
(Intercept)      4.36310    0.05643  77.314  < 2e-16 ***
genotypemutant1 -5.34393    0.81844  -6.529  6.6e-11 ***
genotypemutant2 -1.43190    0.12852 -11.141  < 2e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 463.4789  on 11  degrees of freedom
Residual deviance:   6.5655  on  9  degrees of freedom
AIC: Inf

Number of Fisher Scoring iterations: 5
```
`family=poisson` defines that the GLM should assume Poisson distribution of the data. We see that the **Residual deviance** is 6.5655 on 9 **degrees of freedom**, which is good. If the Residual deviance were much higher than the degrees of freedom, we should adjust the distribution to `family=quasipoisson`. Next, the `Coefficient` lines are interesting. The `(Intercept)` lines show the accuracy of the modelling of the default dataset, here wild.type. A p-value of < 2e-16 is very good: the lower the p-value, the better the model. Next, we see statistical estimations for mutant1 and mutant2, both compared to wild.type, and both are highly significantly different from wild.type.
To compare mutant1 with mutant2 simply use the line `model_glm <- glm(value[5:12]~sample[5:12], family=poisson, data=stat.data)` to select only the rows associated with these two genotypes.

#### GLM assuming binomial distribution (good for datasets with ratio or percentage values)

```
model_glm <- glm(cbind(value,res)~sample, family=binomial, data=data.melt)
``` 
The output:
```R
> summary(model_glm)

Call:
glm(formula = cbind(value, res) ~ sample, family = binomial, 
    data = data.melt)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7597  -0.9177   0.1292   0.4801   2.7572  

Coefficients:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)        1.2950     0.1217  10.641   <2e-16 ***
genotypemutant1   -6.8773     0.8270  -8.316   <2e-16 ***
genotypemutant2   -2.7614     0.1767 -15.628   <2e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 708.504  on 11  degrees of freedom
Residual deviance:  16.599  on  9  degrees of freedom
AIC: 61.954

Number of Fisher Scoring iterations: 5
```
The main difference is that we need to link failure and success values (here value and res) by `cbind(value,res)`, and use `family=binomial`. The output reads the same way as before. We can see that the p-values are even slightly better than with Poisson distribution, which makes sense for this ratio data.
